import MyChart from '../../../src/components/Graphs/scatter';
import LinearRegression from "../../../src/components/Graphs/linear"
import Answer from "../../../src/components/Answer/Answer"
import Equ from "../../../src/components/Equations/index"
import Supple from "./supple"
import GeneralForm from "../../../src/components/01ML_algo_01/general_form"

# 線形回帰(Liner Regression)

それでは早速、もっともシンプルな線形回帰を実装してみましょう。
## 線形回帰とは

線形回帰とは、データの傾向を直線(三次元では平面、それの次元では超平面)で表現する手法です。
つまり、目的変数と説明変数の関係は以下のように表せると仮定して、

$$
\hat{y} = θ_0 + θ_1 x_1 + θ_2 x_2 + ... + θ_n x_n
$$

といったように、目的変数が説明変数の線形結合で表現できると仮定しています。

例えば、身長と体重を測定したデータがあったとします。
このデータを直線で表現すると、以下のようになります

<MyChart/>

このグラフを見てみると、身長が高くなるにつれて体重も増えていることがわかります。
つまり、身長がいくつかが分かれば、体重もある程度予測できるということです。

<LinearRegression />



## 重回帰分析
上の回帰は、身長から体重を予測するという単回帰分析でした。
身長だけでは、体重を予測するのは難しいと思います。そこで、さらに多くの特徴を考慮した上で、体重の予測を行うことを考えます。

:::tip 単回帰分析と重回帰分析
単回帰分析は、身長から体重を予測するなど、一つの目的変数に対して、一つの説明変数を用いる分析です。
一方で、重回帰分析は、説明変数として身長の他に、年齢や年齢など二つ以上の説明変数を用いる分析です。
:::

では早速、重回帰分析に取り掛かりましょう。まずは、簡単な例から見てみましょう。
単回帰では、$y$が体重で$x$が身長となっており、
$$
\hat{y} = a + b x
$$

という関係式になっていましたが、例えば$x$を身長、$z$を年齢として
$$
\hat{y} = a + b x + c z　①
$$

という関係式になります。予測値$\hat{y}$と正解データ$y$の誤差を最小にするように、$a,b,c$を求めます。一般に機械学習では、損失関数を最適化するパラメータ(ここでは、a,b,cのこと)を見つけることを目的としています。線形回帰では、平均二乗誤差を損失関数として用います。
損失関数は以下のようになります。

$$
S(a,b,c) =  \sum_{i=0}^{n-1} (y_i - (a + b x_i + c z_i))^2
$$
これを最小にする$a,b,c$を求めることで、①を満たす$a,b,c$を求めることができます。
では、a,b,cを求める方法を考えていきましょう。

$S(a,b,c)$をa,b,cについてそれぞれ偏微分してみます。

$$
\frac{\partial S}{\partial a} = -2 \sum_{i=0}^{n-1} (y_i - (a + b x_i + c z_i))
$$
$$
\frac{\partial S}{\partial b} = -2 \sum_{i=0}^{n-1} x_i (y_i - (a + b x_i + c z_i))
$$
$$
\frac{\partial S}{\partial c} = -2 \sum_{i=0}^{n-1} z_i (y_i - (a + b x_i + c z_i))
$$

これらを0とおいて、$a,b,c$について解くと、
$$
a = \frac{\sum_{i=0}^{n-1} (y_i - b x_i - c z_i)}{n}
$$
$$
b = \frac{\sum_{i=0}^{n-1} x_i (y_i - a - c z_i)}{\sum_{i=0}^{n-1} x_i^2}
$$
$$
c = \frac{\sum_{i=0}^{n-1} z_i (y_i - a - b x_i)}{\sum_{i=0}^{n-1} z_i^2}
$$
となり、これらを使って、$a,b,c$を求めることができます。

<GeneralForm props="正規方程式の一般形"/>

:::tip 他の損失関数
線形回帰の損失関数としては、平均二乗誤差が最も一般的ですが、他にも様々な損失関数があります。
例えば、平均絶対誤差や、ハブラー損失関数などがあります。
- 平均絶対誤差(Mean Absolute Error, MAE)


予測値と正解データの差の絶対値の平均をとったものです。
$$
S =  \sum_{i=0}^{n-1} |y_i - \hat{y}|
$$
平均絶対誤差は、外れ値の影響を受けにくいという特徴があるので、外れ値が多いデータに対しては、平均二乗誤差よりも平均絶対誤差の方が良い結果を出すことが多いです。

- ハブラー損失関数(Huber Loss)


これは誤差が小さいときは二乗誤差、大きいときは絶対誤差を用いる損失関数です。

$$
L_{\delta}(a) = \begin{cases}
\frac{1}{2}a^2 & \text{for } |a| \leq \delta, \\
\delta (|a| - \frac{1}{2}\delta) & \text{otherwise}.
\end{cases}
$$
ここで、$a = y_i - \hat{y}_i$は誤差、$\delta$は閾値です。
これらの他にも、さまざまな損失がありますが、それぞれのドメインに合わせて、適切な損失関数を選択する必要があります。
:::

## 重回帰分析の実装

それでは、実際に重回帰分析を実装してみましょう。
今回は、sklearnの[California Housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)を用いて、線形回帰の実装を行います。機械学習の一般的な流れに沿って行います。

:::tip California Housing datasetについて
カリフォルニア州の住宅価格のデータセットです。MedInc(地区の収入の中央値)やHouseAge(地区の家の住宅の平均年数)など8つの特徴量から、HouseValue(住宅価格の中央値)を予測するタスクです。
:::

それでは、Pythonで実装していきましょう。
```python
#ライブラリの読み込み
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
```

```python
#データの読み込み
from sklearn.datasets import fetch_california_housing
california = fetch_california_housing(as_frame=True)
```

- as_frame=Trueとすることで、データをpandasのDataFrameとして読み込むことができます。


データの確認をしてみましょう。
```python
#データの確認
frame = california.frame
frame.head()
``` 


|   | MedInc | HouseAge | AveRooms | AveBedrms | Population | AveOccup | Latitude | Longitude | MedHouseVal |
|---|--------|----------|----------|-----------|------------|----------|----------|-----------|-------------|
| 0 | 8.3252 | 41.0     | 6.984127 | 1.023810  | 322.0      | 2.555556 | 37.88    | -122.23   | 4.526       |
| 1 | 8.3014 | 21.0     | 6.238137 | 0.971880  | 2401.0     | 2.109842 | 37.86    | -122.22   | 3.585       |
| 2 | 7.2574 | 52.0     | 8.288136 | 1.073446  | 496.0      | 2.802260 | 37.85    | -122.24   | 3.521       |
| 3 | 5.6431 | 52.0     | 5.817352 | 1.073059  | 558.0      | 2.547945 | 37.85    | -122.25   | 3.413       |
| 4 | 3.8462 | 52.0     | 6.281853 | 1.081081  | 565.0      | 2.181467 | 37.85    | -122.25   | 3.422       |

今回の目的変数は、MedHouseVal(住宅価格の中央値)です。このデータを特徴量と目的変数に分割します。

```python
#データを特徴量と目的変数に分割
X = frame.drop('MedHouseVal', axis=1)
y = frame['MedHouseVal']
```

全てのデータを使って学習したいところですが、全てを学習に使用すると、モデルの精度を
評価する際に、学習に使用したデータを評価に使用してしまうことになります。
これでは、モデルの汎化性能を確認できないので、データを学習用とテスト用に分割します。

```python
#データを学習用とテスト用に分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

データを分割したので、学習を行います。sklearnのLinearRegressionを使って、学習を行います。


```python
#訓練と予測
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

y_predの中身を見てみましょう。
```python
print(y_pred)
```

```
[0.71912284, 1.76401657, 2.70965883, ..., 4.46877017, 1.18751119,
       2.00940251]
```

予測をしたところで、次にモデルの精度の評価を行います。
:::tip モデルの評価
モデルの評価には、様々な指標がありますが、回帰の場合は、平均二乗誤差や平均絶対誤差、決定係数などがよく用いられます。
平均二乗誤差と平均絶対誤差は、損失関数で登場しましたが、モデルの評価にも用いることができます。
:::
平均二乗誤差と絶対平均誤差は、すでに取り上げたので、ここでは決定係数について説明します。実際、回帰の評価には、決定係数がよく用いられます。

### 決定係数(R2 score)

決定係数は、説明変数が目的変数をどれだけ説明するかの指標で、0から1の値を取り、1に近い方がよく説明していると言えます。回帰分析の精度を表すのによく用いられます。
以下の式で表されます。

$$
R^2 = 1 - \frac{\sum_{i=0}^{n-1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n-1} (y_i - \bar{y})^2}
$$

- 分子は残差平方和、分母は全平方和と呼ばれます。
- $y_i$ : i番目の正解データ
- $\hat{y}_i$ : i番目の予測値
- $\bar{y}$ : 正解データの平均値

:::tip 決定係数の気持ち
$R^2$の式を見てみると、分母は、正解データの分散となっています。一方、分子は、正解データの分散から予測値の分散を引いたもの(誤差)になっています。つまり、分子は、予測値の分散がどれだけ正解データの分散を説明できていないかを表しています。これを正解データの分散で割ることで、正解データの分散に対する予測値の分散の割合を表しています。つまり、1からそれを引いた決定係数は、1に近いほど正解データの分散に対する予測値の分散の割合が大きいということになります。つまり、1に近いほど、予測値の分散が正解データの分散を説明できているということになります。
:::

決定係数がわかったところで、先ほどのカリフォルニア州の住宅価格のデータセットの決定係数を計算してみましょう。

```python
#決定係数の計算
from sklearn.metrics import r2_score
r2_score(y_test, y_pred)
```

- 0.57

0.57という値が出ました。これは、モデルがデータの約半分程度しか説明できていないということになります。この値を上げるためには、どうすれば良いでしょうか。以下に、いくつかの方法を挙げます。

- 特徴量エンジニアリング
- データの前処理
- モデルの選択
- ハイパーパラメータの調整

などが挙げられます。しかし、この章はアルゴリズムの解説を目的としているので、詳しくは割愛します。後の章で、これらについて取り上げます。