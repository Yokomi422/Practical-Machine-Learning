import MyChart from '../../../src/components/Graphs/scatter';
import LinearRegression from "../../../src/components/Graphs/linear"
import Answer from "../../../src/components/Answer/Answer"
import Equ from "../../../src/components/Equations/index"
import Supple from "./supple"
import GeneralForm from "../../../src/components/01ML_algo_01/general_form"
import Colab from "../../../src/components/Colab/Colab"

# 線形回帰(Liner Regression)

それでは早速、もっともシンプルな線形回帰を実装してみましょう。
## 線形回帰とは

線形回帰とは、データの傾向を直線(三次元では平面、それの次元では超平面)で表現する手法です。
つまり、目的変数と説明変数の関係は以下のように表せると仮定して、

$$
\hat{y} = θ_0 + θ_1 x_1 + θ_2 x_2 + ... + θ_n x_n
$$

といったように、目的変数が説明変数の線形結合で表現できると仮定しています。

例えば、身長と体重を測定したデータがあったとします。
このデータを直線で表現すると、以下のようになります

<MyChart/>

このグラフを見てみると、身長が高くなるにつれて体重も増えていることがわかります。
つまり、身長がいくつかが分かれば、体重もある程度予測できるということです。

<LinearRegression />



## 重回帰分析
上の回帰は、身長から体重を予測するという単回帰分析でした。
身長だけでは、体重を予測するのは難しいと思います。そこで、さらに多くの特徴を考慮した上で、体重の予測を行うことを考えます。

:::tip 単回帰分析と重回帰分析
単回帰分析は、身長から体重を予測するなど、一つの目的変数に対して、一つの説明変数を用いる分析です。
一方で、重回帰分析は、説明変数として身長の他に、年齢や年齢など二つ以上の説明変数を用いる分析です。
:::

では早速、重回帰分析に取り掛かりましょう。まずは、簡単な例から見てみましょう。
単回帰では、$y$が体重で$x$が身長となっており、
$$
\hat{y} = a + b x
$$

という関係式になっていましたが、例えば$x$を身長、$z$を年齢として
$$
\hat{y} = a + b x + c z　①
$$

という関係式になります。予測値$\hat{y}$と正解データ$y$の誤差を最小にするように、$a,b,c$を求めます。一般に機械学習では、損失関数を最適化するパラメータ(ここでは、a,b,cのこと)を見つけることを目的としています。線形回帰では、平均二乗誤差を損失関数として用います。
損失関数は以下のようになります。

$$
S(a,b,c) =  \sum_{i=0}^{n-1} (y_i - (a + b x_i + c z_i))^2
$$
これを最小にする$a,b,c$を求めることで、①を満たす$a,b,c$を求めることができます。
では、a,b,cを求める方法を考えていきましょう。

$S(a,b,c)$をa,b,cについてそれぞれ偏微分してみます。

$$
\frac{\partial S}{\partial a} = -2 \sum_{i=0}^{n-1} (y_i - (a + b x_i + c z_i))
$$
$$
\frac{\partial S}{\partial b} = -2 \sum_{i=0}^{n-1} x_i (y_i - (a + b x_i + c z_i))
$$
$$
\frac{\partial S}{\partial c} = -2 \sum_{i=0}^{n-1} z_i (y_i - (a + b x_i + c z_i))
$$

これらを0とおいて、$a,b,c$について解くと、
$$
a = \frac{\sum_{i=0}^{n-1} (y_i - b x_i - c z_i)}{n}
$$
$$
b = \frac{\sum_{i=0}^{n-1} x_i (y_i - a - c z_i)}{\sum_{i=0}^{n-1} x_i^2}
$$
$$
c = \frac{\sum_{i=0}^{n-1} z_i (y_i - a - b x_i)}{\sum_{i=0}^{n-1} z_i^2}
$$
となり、これらを使って、$a,b,c$を求めることができます。

<GeneralForm props="正規方程式の一般形"/>

:::tip 他の損失関数
線形回帰の損失関数としては、平均二乗誤差が最も一般的ですが、他にも様々な損失関数があります。
例えば、平均絶対誤差や、ハブラー損失関数などがあります。
- 平均絶対誤差(Mean Absolute Error, MAE)


予測値と正解データの差の絶対値の平均をとったものです。
$$
S =  \sum_{i=0}^{n-1} |y_i - \hat{y}|
$$
平均絶対誤差は、外れ値の影響を受けにくいという特徴があるので、外れ値が多いデータに対しては、平均二乗誤差よりも平均絶対誤差の方が良い結果を出すことが多いです。

- ハブラー損失関数(Huber Loss)


これは誤差が小さいときは二乗誤差、大きいときは絶対誤差を用いる損失関数です。

$$
L_{\delta}(a) = \begin{cases}
\frac{1}{2}a^2 & \text{for } |a| \leq \delta, \\
\delta (|a| - \frac{1}{2}\delta) & \text{otherwise}.
\end{cases}
$$
ここで、$a = y_i - \hat{y}_i$は誤差、$\delta$は閾値です。
これらの他にも、さまざまな損失がありますが、それぞれのドメインに合わせて、適切な損失関数を選択する必要があります。
:::

## 重回帰分析の実装

それでは、実際に重回帰分析を実装してみましょう。
今回は、sklearnの[California Housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)を用いて、線形回帰の実装を行います。機械学習の一般的な流れに沿って行います。

:::tip California Housing datasetについて
カリフォルニア州の住宅価格のデータセットです。MedInc(地区の収入の中央値)やHouseAge(地区の家の住宅の平均年数)など8つの特徴量から、HouseValue(住宅価格の中央値)を予測するタスクです。
:::

それでは、Pythonで実装していきましょう。

Pythonのコードの実行結果は、以下のGoogle Colaboratoryで確認することができます。

<Colab url="https://colab.research.google.com/github/Yokomi422/Practical-Machine-Learning/blob/main/docs/01-Machine-learning-algorithms/01-linear-regression/colab01.ipynb" />

上のコードで、線形回帰のモデルを作成し、学習させることができました。

しかし、上のコードの精度は、0.57という値でした。これは、モデルがデータの約半分程度しか説明できていないということになります。この値を上げるためには、どうすれば良いでしょうか。以下に、いくつかの方法を挙げます。

- 特徴量エンジニアリング
- データの前処理
- モデルの選択
- ハイパーパラメータの調整


などが挙げられます。しかし、この章はアルゴリズムの解説を目的としているので、詳しくは割愛します。後の章で、これらについて取り上げます。