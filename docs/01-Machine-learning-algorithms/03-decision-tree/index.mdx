import Tree01 from "../../../src/components/01ML_algo/algo03/Tree01/Tree01"
import Tree02 from "../../../src/components/01ML_algo/algo03/Tree02/Tree02"
import Graph01 from "../../../src/components/01ML_algo/algo03/Graph01/Graph01"

# 決定木(Decision Tree)

## 決定木の概要

これから決定木について、学習していきます。
決定木は、分類問題と回帰問題の両方に使われる機械学習の汎用的な手法です。
天気とアウトドア活動の参加に関する決定木の例を用いて説明します。

<Tree01 />

この木構造というデータ構造は、頂点（ノード）と枝（エッジ）からなります。
一番上にあるノードを特に、根（ルート）と呼びます。そして、下に向かって枝が伸びていないノードを葉（リーフ）と呼びます。

決定木は、IF ~ THENルーチンという条件分岐の構造を持っています。
この図の例では、根のノードにある「天気は晴れか？」という質問に対して、Yesならば左の枝を、Noならば右の枝を辿るようになっています。

:::tip 回帰問題
上の例では、分類問題の例を示しましたが、回帰問題にも決定木は使われます。
例えば、会員の継続期間の予測という回帰問題を考えてみましょう。

<Tree02/>

このように、決定木は、データを分割することで、回帰問題を解くことができます。
:::

## 損失関数

線形回帰と多項式回帰の損失関数は、平均二乗和誤差でした。
決定木の損失関数は、分類問題と回帰問題で異なります。
まず、分類問題の損失関数を見てみましょう。

### ジニ不純物(Gini impurity)
ジニ不純物は、あるグループの中で、異なるクラスのデータがどれだけ混ざっているかを表す指標です。
ジニ不純物が小さいほど、あるグループの中で、同じクラスのデータが多く、
ジニ不純物が大きいほど、あるグループの中で、異なるクラスのデータが多いことを表します。

ジニ不純物は以下のように定義されます。

$$$
\text{Gini(p)} = 1 - \sum_{i=1}^{n} p_i^2
$$$
$$$
p_i = \text{各クラスの割合}
$$$

:::tip 例1
あるグループの中に、赤いボールが3個、青いボールが2個あるとします。
このとき、ジニ不純物は以下のように計算できます。
$$$
\text{Gini(p)} = 1 - \left(\frac{3}{5}\right)^2 - \left(\frac{2}{5}\right)^2 = 0.48
$$$
:::

:::tip 例2
あるグループの中に、青いボールが10個あるとします。
この時のジニ不純物は以下のように計算できます。
$$$
\text{Gini(p)} = 1 - \left(\frac{10}{10}\right)^2 = 0
$$$
:::

このようにジニ不純物は、ある一つのグループがどれくらいの種類からなるかを表します。
この指標を使って、決定木の損失関数を定義します。
ジニ不純物それ自体は、一つのノードの中のデータの分布を表す指標ですが、
決定木の損失関数として使う場合は、全てのノードのジニ不純物の重み付き平均をとります。

つまり、ある決定木のジニ不純物を$G$とすると、決定木の損失関数は以下のように定義されます。

$$$
\text{G} = \sum_{i=1}^{n} \frac{N_i}{N} \text{Gini(p)}
$$$
$$$
\text{N} = \text{全データ数}
$$$
$$$
\text{N}_i = \text{各ノードのデータ数}
$$$

:::tip 例
Node1には50個のサンプルがあり、そのうち30個がクラスA、20個がクラスBに属しています。
Node2には100個のサンプルがあり、そのうち40個がクラスA、60個がクラスBに属しています。
このとき、決定木の損失関数は以下のように計算できます。
$$$
\text{G} = \frac{50}{150} \times 0.48 + \frac{100}{150} \times 0.48 = 0.48
$$$
:::