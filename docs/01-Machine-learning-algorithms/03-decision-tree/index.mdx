import Tree01 from "../../../src/components/01ML_algo/algo03/Tree01/Tree01"
import Tree02 from "../../../src/components/01ML_algo/algo03/Tree02/Tree02"
import Graph01 from "../../../src/components/01ML_algo/algo03/Graph01/Graph01"

# 決定木(Decision Tree)

## 決定木の概要

これから決定木について、学習していきます。
決定木は、分類問題と回帰問題の両方に使われる機械学習の汎用的な手法です。
天気とアウトドア活動の参加に関する決定木の例を用いて説明します。

<Tree01 />

この木構造というデータ構造は、頂点（ノード）と枝（エッジ）からなります。
一番上にあるノードを特に、根（ルート）と呼びます。そして、下に向かって枝が伸びていないノードを葉（リーフ）と呼びます。

決定木は、IF ~ THENルーチンという条件分岐の構造を持っています。
この図の例では、根のノードにある「天気は晴れか？」という質問に対して、Yesならば左の枝を、Noならば右の枝を辿るようになっています。

:::tip 回帰問題
上の例では、分類問題の例を示しましたが、回帰問題にも決定木は使われます。
例えば、会員の継続期間の予測という回帰問題を考えてみましょう。

<Tree02/>

このように、決定木は、データを分割することで、回帰問題を解くことができます。
:::

## 決定木の仕組み

決定木の概要がわかったところで、実際に決定木を実装してみましょう。
決定木の実装には、
-  ①どの特徴量を選択するか
- ②どの深さまで木を成長させるか

が重要です。
①について詳しく見ていきましょう。
これは、各ノードでの分割に使用する特徴量を選択することを意味します。
これは、ジニ不純物やエントロピーという指標を用いた情報利得という指標を用いて、
各特徴量の分割の良さを評価し、最も良い分割を選択します。
ひとまず、ジニ不純物とエントロピーについて説明します。

### ジニ不純物(Gini impurity)
ジニ不純物は、あるグループの中で、異なるクラスのデータがどれだけ混ざっているかを表す指標です。
ジニ不純物が小さいほど、あるグループの中で、同じクラスのデータが多く、
ジニ不純物が大きいほど、あるグループの中で、異なるクラスのデータが多いことを表します。

ジニ不純物は以下のように定義されます。

$$$
\text{Gini(p)} = 1 - \sum_{i=1}^{n} p_i^2
$$$
$$$
p_i = \text{各クラスの割合}
$$$

:::tip 例1
あるグループの中に、赤いボールが3個、青いボールが2個あるとします。
このとき、ジニ不純物は以下のように計算できます。
$$$
\text{Gini(p)} = 1 - \left(\frac{3}{5}\right)^2 - \left(\frac{2}{5}\right)^2 = 0.48
$$$
:::

:::tip 例2
あるグループの中に、青いボールが10個あるとします。
この時のジニ不純物は以下のように計算できます。
$$$
\text{Gini(p)} = 1 - \left(\frac{10}{10}\right)^2 = 0
$$$
:::

このようにジニ不純物は、ある一つのグループがどれくらいの種類からなるかを表します。

### エントロピー(Entropy)
ジニ不純物の他にデータの乱雑さを表す指標として、エントロピーがあります。
エントロピーは、以下のように定義されます。
$$$
\text{Entropy(p)} = - \sum_{i=1}^{n} p_i \log_2 p_i
$$$
$$$
p_i = \text{各クラスの割合}
$$$
$$$
\text{n} = \text{全データ数}
$$$

エントロピーもジニ不純物と同様に、ある一つのグループがどれくらいの種類からなるかを表します。
エントロピーが小さいほど、あるグループの中で、同じクラスのデータが多く、
エントロピーが大きいほど、あるグループの中で、異なるクラスのデータが多いことを表します。

:::tip 例1
あるグループの中に、赤いボールが3個、青いボールが2個あるとします。 このとき、エントロピーは以下のように計算できます。
$$$
\text{Entropy(p)} = - \left(\frac{3}{5} \log_2 \frac{3}{5} + \frac{2}{5} \log_2 \frac{2}{5}\right) = 0.971
$$$
:::

:::tip 例2
あるグループの中に、青いボールが10個あるとします。
この時のエントロピーは以下のように計算できます。
$$$
\text{Entropy(p)} = - \left(\frac{10}{10} \log_2 \frac{10}{10}\right) = 0
$$$
:::

### 情報利得(Information gain)

ジニ不純物とエントロピーの指標を用いて、各特徴量の分割の良さを評価する指標として、情報利得があります。
情報利得は、以下のように定義されます。
ジニ不純物を用いると、
$$$
\text{Information gain} = \text{親ノードのジニ不純物} - \text{子ノードのジニ不純物の重み付き平均}
$$$
エントロピーを用いると、
$$$
\text{Information gain} = \text{親ノードのエントロピー} - \text{子ノードのエントロピーの重み付き平均}
$$$

となります。  
さて、この指標が最適な分割を選択するための指標となるのは、なぜでしょうか？
これは、情報利得が大きいほど、親ノードと子ノードの差が大きくなる(子ノードのデータの複雑さが小さくなる)ため、
親ノードと子ノードの差が大きいほど、分割が良いということになります。
決定木は、情報利得が最大となるように、特徴量を選択していきます。

次は②どの深さまで木を成長させるかを見ていきましょう。

### 決定木の深さ

決定木は木の深さをデータの総数以上にすると、必ず全てのデータを分類できるようになります。
しかし、これは過学習を引き起こしています。
決定木の深さを調整するには、今までのように数式を用いた方法ではなく、
交差検証(のちに扱います)や、木の深さを調整するパラメータを用いる方法があります。早期停止や正則化もある。
